# ğŸ¤– RAG-Based PDF Chatbot

A full-stack Retrieval-Augmented Generation (RAG) system to chat with PDF documents

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FE6F61?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![FAISS](https://img.shields.io/badge/FAISS-FF6B35?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHZpZXdCb3g9IjAgMCA0MCA0MCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMjAiIGN5PSIyMCIgcj0iMjAiIGZpbGw9IiNGRjZCMzUiLz4KPC9zdmc+Cg==)](https://faiss.ai/)

An end-to-end GenAI application that allows users to upload PDF documents and ask natural-language questions about their content. The system uses Retrieval-Augmented Generation (RAG) with FAISS vector search and LLM-based answering, exposed via a Flask API and consumed through a Streamlit chat interface.

## âœ¨ Key Features

- ğŸ“„ Upload and chat with PDF documents
- ğŸ§  Retrieval-Augmented Generation (RAG) pipeline
- ğŸ” Semantic search using FAISS
- âŒ Hallucination control (answers only from document context)
- ğŸ’¬ ChatGPT-style conversational UI
- ğŸ§± Clean modular codebase
- âš¡ Fast LLM inference using Groq

## ğŸ—ï¸ System Architecture

User
â†“
Streamlit Frontend (UI)
â†“ HTTP POST (PDF + Query)
Flask Backend (API)
â†“
RAG Pipeline
â”œâ”€â”€ PDF Text Extraction
â”œâ”€â”€ Text Cleaning
â”œâ”€â”€ Chunking
â”œâ”€â”€ Embedding Generation
â”œâ”€â”€ FAISS Vector Search
â”œâ”€â”€ Context Construction
â”œâ”€â”€ Prompt Engineering
â””â”€â”€ LLM Answer Generation

text

## ğŸ“ Project Structure

RAG/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ app.py # Flask backend API
â”‚
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ build_context.py # Context construction
â”‚ â”œâ”€â”€ chunk_text.py # Text chunking logic
â”‚ â”œâ”€â”€ clean_text.py # Text cleaning
â”‚ â”œâ”€â”€ embeddings.py # Embedding generation
â”‚ â”œâ”€â”€ extract_text.py # PDF text extraction
â”‚ â”œâ”€â”€ llm.py # Groq LLM interface
â”‚ â”œâ”€â”€ prompt.py # Prompt construction
â”‚ â””â”€â”€ vectordb.py # FAISS vector database
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ streamlit_app.py # Streamlit chat UI
â”‚
â”œâ”€â”€ rag_pipeline.py # Orchestrates full RAG flow
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env # Environment variables
â””â”€â”€ README.md

text

## ğŸ› ï¸ Technology Stack

### Backend
- **Python** - Core language
- **Flask** â€“ REST API layer
- **FAISS** â€“ Vector similarity search
- **Sentence-Transformers** â€“ Text embeddings
- **Groq LLM** â€“ Language model inference
- **PyPDF2** â€“ PDF text extraction

### Frontend
- **Streamlit** â€“ Interactive chat interface

## âš™ï¸ How the RAG Pipeline Works

1. **PDF Upload** - User uploads a PDF through the Streamlit UI
2. **Text Extraction & Cleaning** - Raw text is extracted and normalized
3. **Chunking** - Text is split into overlapping chunks for better retrieval
4. **Embedding Generation** - Each chunk is converted into a dense vector
5. **Vector Search (FAISS)** - Query embedding is matched with top-K relevant chunks
6. **Context Construction** - Retrieved chunks are combined into a single context
7. **Prompt Engineering** - A controlled prompt ensures document-grounded answers
8. **LLM Answer Generation** - Groq LLM generates the final response

## â–¶ï¸ How to Run the Application

### 1ï¸âƒ£ Create Virtual Environment
```bash
python -m venv myenv
source myenv/bin/activate   # macOS/Linux
myenv\Scripts\activate      # Windows
2ï¸âƒ£ Install Dependencies
bash
pip install -r requirements.txt
3ï¸âƒ£ Set Environment Variable
bash
export GROQ_API_KEY="your_api_key_here"   # macOS/Linux
setx GROQ_API_KEY "your_api_key_here"     # Windows
4ï¸âƒ£ Run Flask Backend
bash
cd RAG
python backend/app.py
Backend runs at: http://127.0.0.1:5000

5ï¸âƒ£ Run Streamlit Frontend
bash
cd RAG
streamlit run frontend/streamlit_app.py
Frontend opens at: http://localhost:8501

ğŸ§ª Example Questions
What is the project about?

Explain the milestones.

What technologies are used?

What problem does this system solve?

ğŸ”’ Hallucination Control
Answers are generated only from retrieved document context. If the information is missing, the system responds with:

Not found in the document.

This ensures trustworthy and reliable answers.

ğŸ¯ Use Cases
Research paper analysis

Academic document summarization

Technical documentation Q&A

Knowledge-base search systems

ğŸ“Œ Future Enhancements
ğŸ“ Source citations with chunk references

âš¡ Vector DB caching per PDF

ğŸ³ Dockerization

ğŸŒ Cloud deployment

ğŸ‘¥ Multi-user support

ğŸ§  Interview Highlight
Designed and implemented a modular RAG system using FAISS and LLMs, deployed via a Flask API and Streamlit UI with strict hallucination control.

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

â­ Show Your Support
If you found this project useful, please give it a â­ on GitHub!

ğŸš€ Built with â¤ï¸ for intelligent document understanding
Made by Varshith Reddy